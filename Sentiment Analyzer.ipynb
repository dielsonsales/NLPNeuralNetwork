{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import os\n",
    "import pathlib\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from spacy.compat import pickle\n",
    "\n",
    "pt_br_model = 'pt_wikipedia_md'\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        \"\"\"\n",
    "        Loads the language model file.\n",
    "        \"\"\"\n",
    "        with (path / 'config.json').open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / 'model').open('rb') as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000, n_threads=2):\n",
    "        for doc in docs:\n",
    "            Xs = get_features([doc], self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for doc, label in zip([doc], ys):\n",
    "                doc.sentiment = label\n",
    "            yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        sentences.append(doc)\n",
    "        labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype='int32')\n",
    "\n",
    "\n",
    "def get_features(docs: list, max_length):\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings]\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'], recurrent_dropout=settings['dropout'], dropout=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='relu'))\n",
    "    model.add(Dropout(settings['dropout']))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "          lstm_shape, lstm_settings, lstm_optimizer, batch_size=100,\n",
    "          nb_epoch=5, by_sentence=True):\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load(pt_br_model)\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    print(\"Starting get_labelled_sentences()\")\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "    model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),\n",
    "              epochs=nb_epoch, batch_size=batch_size)\n",
    "    print(\"Model ready\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(pt_br_model)\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):\n",
    "        if doc.sentiment >= 0.5 and bool(labels[i]) is True:\n",
    "            print(texts[i] + \" correctly classified as True \" + str(doc.sentiment))\n",
    "        elif doc.sentiment >= 0.5 and bool(labels[i]) is False:\n",
    "            print(texts[i] + \" wrongly classified as True \" + str(doc.sentiment))\n",
    "        elif doc.sentiment < 0.5 and bool(labels[i]) is True:\n",
    "            print(texts[i] + \" wrongly classified as False \" + str(doc.sentiment))\n",
    "        elif doc.sentiment < 0.5 and bool(labels[i]) is False:\n",
    "            print(texts[i] + \" correctly classified as False \" + str(doc.sentiment))\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "train_samples = [\n",
    "    ('bom', 1),\n",
    "    ('perfeito', 1),\n",
    "    ('rápido', 1),\n",
    "    ('ruim', 0),\n",
    "    ('não', 0),\n",
    "    ('melhorar', 0),\n",
    "    ('demorado', 0),\n",
    "    ('droga', 0)\n",
    "]\n",
    "\n",
    "validation_samples = [\n",
    "    ('bom proposta', 1),\n",
    "    ('servir perfeitamente', 1),\n",
    "    ('não gostar', 0),\n",
    "    ('produto ruim', 0)\n",
    "]\n",
    "\n",
    "test_samples = [\n",
    "    ('bom', 1),\n",
    "    ('empresa boa', 1),\n",
    "    ('pode melhorar', 0),\n",
    "    ('não gostei', 0)\n",
    "]\n",
    "\n",
    "\n",
    "def execute(model_dir=None, train_dir=None, dev_dir=None,\n",
    "            is_runtime=False,\n",
    "            nr_hidden=64, max_length=100, # Shape\n",
    "            dropout=0.5, learn_rate=0.001, # General NN config\n",
    "            nb_epoch=5, batch_size=256, nr_examples=-1):  # Training params\n",
    "    train_texts = [item[0] for item in train_samples]\n",
    "    train_labels = [item[1] for item in train_samples]\n",
    "    val_texts = [item[0] for item in validation_samples]\n",
    "    val_labels = [item[1] for item in validation_samples]\n",
    "    \n",
    "    \n",
    "    if model_dir is not None:\n",
    "        if not os.path.exists(model_dir):\n",
    "          os.makedirs(model_dir)\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if is_runtime:\n",
    "        test_texts = [item[0] for item in test_samples];\n",
    "        test_labels = [item[1] for item in test_samples];\n",
    "        acc = evaluate(model_dir, test_texts, test_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        print(\"Training neural network...\")\n",
    "        train_labels = numpy.asarray(train_labels, dtype='int')\n",
    "        val_labels = numpy.asarray(val_labels, dtype='int')\n",
    "        lstm = train(train_texts, train_labels, val_texts, val_labels,\n",
    "                     {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 1},\n",
    "                     {'dropout': dropout, 'lr': learn_rate},\n",
    "                     {}, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / 'model').open('wb') as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / 'config.json').open('w') as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "def train_network():\n",
    "\texecute(model_dir='binary_classification', is_runtime=False, nb_epoch=25, nr_hidden=128)\n",
    "\n",
    "\n",
    "def evaluate_network():\n",
    "\texecute(model_dir='binary_classification', is_runtime=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "Loading spaCy\n",
      "Parsing texts...\n",
      "Starting get_labelled_sentences()\n",
      "Train on 8 samples, validate on 4 samples\n",
      "Epoch 1/25\n",
      "8/8 [==============================] - 3s 417ms/step - loss: 0.7029 - acc: 0.6250 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/25\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.6805 - acc: 0.6250 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 3/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6568 - acc: 0.6250 - val_loss: 0.6991 - val_acc: 0.5000\n",
      "Epoch 4/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.6420 - acc: 0.6250 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 5/25\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.8554 - acc: 0.6250 - val_loss: 0.7056 - val_acc: 0.5000\n",
      "Epoch 6/25\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.5875 - acc: 0.6250 - val_loss: 0.7093 - val_acc: 0.5000\n",
      "Epoch 7/25\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.6595 - acc: 0.6250 - val_loss: 0.7137 - val_acc: 0.5000\n",
      "Epoch 8/25\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.5849 - acc: 0.6250 - val_loss: 0.7196 - val_acc: 0.5000\n",
      "Epoch 9/25\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.6205 - acc: 0.6250 - val_loss: 0.7255 - val_acc: 0.5000\n",
      "Epoch 10/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6539 - acc: 0.6250 - val_loss: 0.7302 - val_acc: 0.5000\n",
      "Epoch 11/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7950 - acc: 0.6250 - val_loss: 0.7283 - val_acc: 0.5000\n",
      "Epoch 12/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.8347 - acc: 0.6250 - val_loss: 0.7201 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6079 - acc: 0.6250 - val_loss: 0.7146 - val_acc: 0.5000\n",
      "Epoch 14/25\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.6632 - acc: 0.6250 - val_loss: 0.7087 - val_acc: 0.5000\n",
      "Epoch 15/25\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.6406 - acc: 0.6250 - val_loss: 0.7049 - val_acc: 0.5000\n",
      "Epoch 16/25\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.6942 - acc: 0.6250 - val_loss: 0.6995 - val_acc: 0.5000\n",
      "Epoch 17/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6910 - acc: 0.6250 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 18/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6255 - acc: 0.6250 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 19/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6635 - acc: 0.6250 - val_loss: 0.6914 - val_acc: 0.5000\n",
      "Epoch 20/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6400 - acc: 0.6250 - val_loss: 0.6902 - val_acc: 0.5000\n",
      "Epoch 21/25\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.6540 - acc: 0.6250 - val_loss: 0.6900 - val_acc: 0.5000\n",
      "Epoch 22/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.7175 - acc: 0.6250 - val_loss: 0.6891 - val_acc: 0.5000\n",
      "Epoch 23/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6799 - acc: 0.6250 - val_loss: 0.6877 - val_acc: 0.5000\n",
      "Epoch 24/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6502 - acc: 0.6250 - val_loss: 0.6866 - val_acc: 0.5000\n",
      "Epoch 25/25\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.6954 - acc: 0.6250 - val_loss: 0.6858 - val_acc: 0.5000\n",
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bom wrongly classified as False 0.4780743420124054\n",
      "empresa boa wrongly classified as False 0.4511517882347107\n",
      "pode melhorar correctly classified as False 0.44887104630470276\n",
      "não gostei correctly classified as False 0.4558592140674591\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "evaluate_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
