{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy\n",
    "import os\n",
    "import pathlib\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from spacy.compat import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pt_br_model = 'pt_wikipedia_md'\n",
    "\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        \"\"\"\n",
    "        Loads the language model file.\n",
    "        \"\"\"\n",
    "        with (path / 'config.json').open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / 'model').open('rb') as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000, n_threads=2):\n",
    "        for doc in docs:\n",
    "            Xs = get_features([doc], self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for doc, label in zip([doc], ys):\n",
    "                doc.sentiment += label\n",
    "            yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        sentences.append(doc)\n",
    "        labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype='int32')\n",
    "\n",
    "\n",
    "def get_features(docs: list, max_length):\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        for token in doc:\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    return Xs\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=False,\n",
    "            weights=[embeddings]\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Dropout(settings['dropout']))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'], recurrent_dropout=settings['dropout'], dropout=settings['dropout'])))\n",
    "    model.add(Dropout(settings['dropout']))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(shape['nr_class'], activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='mse', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "          lstm_shape, lstm_settings, lstm_optimizer, batch_size=100,\n",
    "          nb_epoch=5, by_sentence=True):\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = spacy.load(pt_br_model)\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    print(\"Starting get_labelled_sentences()\")\n",
    "    if by_sentence:\n",
    "        train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "    history = model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels), epochs=nb_epoch, batch_size=batch_size, verbose=0)\n",
    "    print(\"Model ready\")\n",
    "\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('accuracy')\n",
    "    plt.ylabel('acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(pt_br_model)\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'), first=True)\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):\n",
    "        if doc.sentiment >= 0.5 and bool(labels[i]) is True:\n",
    "            print(texts[i] + \" correctly classified as True \" + str(doc.sentiment))\n",
    "        elif doc.sentiment >= 0.5 and bool(labels[i]) is False:\n",
    "            print(texts[i] + \" wrongly classified as True \" + str(doc.sentiment))\n",
    "        elif doc.sentiment < 0.5 and bool(labels[i]) is True:\n",
    "            print(texts[i] + \" wrongly classified as False \" + str(doc.sentiment))\n",
    "        elif doc.sentiment < 0.5 and bool(labels[i]) is False:\n",
    "            print(texts[i] + \" correctly classified as False \" + str(doc.sentiment))\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "train_samples = [\n",
    "    ('bom', 1),\n",
    "    ('perfeito', 1),\n",
    "    ('entrega rápido', 1),\n",
    "    ('satisfeito', 1),\n",
    "    ('excelente', 1),\n",
    "    ('bom qualidade', 1),\n",
    "    ('excelente qualidade', 1),\n",
    "    ('valer pena', 1),\n",
    "    ('melhorar ultimamente', 1),\n",
    "    ('gostar', 1),\n",
    "    ('barato', 1),\n",
    "    ('servir bem', 1),\n",
    "    ('feliz', 1),\n",
    "    ('filho feliz', 1),\n",
    "    ('esposa gostar', 1),\n",
    "    ('suporte bom', 1),\n",
    "    ('ruim', 0),\n",
    "    ('péssimo', 0),\n",
    "    ('quebrado', 0),\n",
    "    ('quebrar', 0),\n",
    "    ('não gostar', 0),\n",
    "    ('filho não gostar', 0),\n",
    "    ('precisar melhorar', 0),\n",
    "    ('poder melhorar', 0),\n",
    "    ('demorar', 0),\n",
    "    ('entrega demorado', 0),\n",
    "    ('droga', 0),\n",
    "    ('mal qualidade', 0),\n",
    "    ('caro', 0),\n",
    "    ('retornar', 0),\n",
    "    ('não servir', 0),\n",
    "    ('ultrapassado', 0),\n",
    "    ('desapontado', 0),\n",
    "    ('triste', 0),\n",
    "    ('precisar suporte', 0)\n",
    "]\n",
    "\n",
    "validation_samples = [\n",
    "    ('bom proposta', 1),\n",
    "    ('servir perfeitamente', 1),\n",
    "    ('acabamento perfeito', 1),\n",
    "    ('qualidade impressionante', 1),\n",
    "    ('marca confiança', 1),\n",
    "    ('querer outro', 1),\n",
    "    ('não gostar resultado', 0),\n",
    "    ('dever melhorar concorrência', 0),\n",
    "    ('insatisfeito produto', 0),\n",
    "    ('custar caro', 0),\n",
    "    ('defeituoso', 0),\n",
    "    ('esperar mais', 0),\n",
    "    ('acionar suporte', 0)\n",
    "]\n",
    "\n",
    "test_samples = [\n",
    "    ('produto bom', 1),\n",
    "    ('empresa bom', 1),\n",
    "    ('valer pena', 1),\n",
    "    ('orgulho empresa', 1),\n",
    "    ('esposa gostar produto', 1),\n",
    "    ('poder melhorar bastante', 0),\n",
    "    ('não gostar produto', 0),\n",
    "    ('quebrar primeiro dia', 0),\n",
    "    ('atendimento péssimo', 0),\n",
    "    ('decepcionado', 0),\n",
    "    ('suporte péssimo', 0)\n",
    "]\n",
    "\n",
    "\n",
    "def execute(model_dir=None, train_dir=None, dev_dir=None,\n",
    "            is_runtime=False,\n",
    "            nr_hidden=64, max_length=100, # Shape\n",
    "            dropout=0.5, learn_rate=0.001, # General NN config\n",
    "            nb_epoch=5, batch_size=256, nr_examples=-1):  # Training params\n",
    "    train_texts = [item[0] for item in train_samples]\n",
    "    train_labels = [item[1] for item in train_samples]\n",
    "    val_texts = [item[0] for item in validation_samples]\n",
    "    val_labels = [item[1] for item in validation_samples]\n",
    "    \n",
    "    \n",
    "    if model_dir is not None:\n",
    "        if not os.path.exists(model_dir):\n",
    "          os.makedirs(model_dir)\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if is_runtime:\n",
    "        test_texts = [item[0] for item in test_samples];\n",
    "        test_labels = [item[1] for item in test_samples];\n",
    "        acc = evaluate(model_dir, test_texts, test_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        print(\"Training neural network...\")\n",
    "        train_labels = numpy.asarray(train_labels, dtype='int')\n",
    "        val_labels = numpy.asarray(val_labels, dtype='int')\n",
    "        lstm = train(train_texts, train_labels, val_texts, val_labels,\n",
    "                     {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 1},\n",
    "                     {'dropout': dropout, 'lr': learn_rate},\n",
    "                     {}, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / 'model').open('wb') as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / 'config.json').open('w') as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "def train_network():\n",
    "    execute(model_dir='binary_classification', is_runtime=False, nb_epoch=175)\n",
    "\n",
    "\n",
    "def evaluate_network():\n",
    "    execute(model_dir='binary_classification', is_runtime=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n",
      "Loading spaCy\n",
      "Parsing texts...\n",
      "Starting get_labelled_sentences()\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produto bom correctly classified as True 0.9069179892539978\n",
      "empresa bom correctly classified as True 0.9866045117378235\n",
      "valer pena correctly classified as True 0.9872559309005737\n",
      "orgulho empresa correctly classified as True 0.7336931228637695\n",
      "esposa gostar produto correctly classified as True 0.9945662021636963\n",
      "poder melhorar bastante correctly classified as False 0.0012498717987909913\n",
      "não gostar produto correctly classified as False 0.0002506551973056048\n",
      "quebrar primeiro dia correctly classified as False 0.015570004470646381\n",
      "atendimento péssimo correctly classified as False 0.014743581414222717\n",
      "decepcionado correctly classified as False 0.08269061148166656\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
